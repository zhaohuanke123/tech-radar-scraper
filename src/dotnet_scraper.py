"""
dotnet_scraper.py - .NET å®˜æ–¹åšå®¢ RSS è§£ææ¨¡å—
æ•°æ®æº: https://devblogs.microsoft.com/dotnet/feed/
åŠŸèƒ½: æ‹‰å– RSS è®¢é˜…ï¼Œè¿‡æ»¤æœ€è¿‘ 48 å°æ—¶å†…å‘å¸ƒçš„æ–‡ç« ï¼Œæå–æ ‡é¢˜ã€é“¾æ¥ã€æ—¶é—´ä¸æ‘˜è¦
"""

import feedparser
import logging
from datetime import datetime, timezone, timedelta
from email.utils import parsedate_to_datetime
from html.parser import HTMLParser

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# RSS æ•°æ®æºåœ°å€
DOTNET_FEED_URL = "https://devblogs.microsoft.com/dotnet/feed/"

# æŠ“å–æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤æŠ“å–æœ€è¿‘ 48 å°æ—¶å†…çš„æ–‡ç« 
HOURS_WINDOW = 48


class _MLStripper(HTMLParser):
    """ç®€å•çš„ HTML æ ‡ç­¾å‰¥ç¦»å™¨ï¼Œå°†æ‘˜è¦ä¸­çš„ HTML æ ‡ç­¾æ¸…é™¤ä¸ºçº¯æ–‡æœ¬"""

    def __init__(self):
        super().__init__()
        self.reset()
        self.fed: list[str] = []

    def handle_data(self, d: str):
        self.fed.append(d)

    def get_data(self) -> str:
        return " ".join(self.fed)


def _strip_html(html: str) -> str:
    """å»é™¤å­—ç¬¦ä¸²ä¸­çš„ HTML æ ‡ç­¾ï¼Œè¿”å›çº¯æ–‡æœ¬"""
    stripper = _MLStripper()
    stripper.feed(html)
    return stripper.get_data().strip()


def _truncate(text: str, max_length: int = 200) -> str:
    """å°†æ–‡æœ¬æˆªæ–­è‡³æŒ‡å®šé•¿åº¦ï¼Œè¶…å‡ºéƒ¨åˆ†ç”¨çœç•¥å·ä»£æ›¿"""
    return text if len(text) <= max_length else text[:max_length].rstrip() + "â€¦"


def fetch_dotnet_articles(hours: int = HOURS_WINDOW) -> list[dict]:
    """
    æ‹‰å–å¹¶è§£æ .NET åšå®¢ RSSï¼Œè¿”å›æŒ‡å®šæ—¶é—´çª—å£å†…çš„æ–‡ç« åˆ—è¡¨ã€‚

    Args:
        hours: æŠ“å–å¤šå°‘å°æ—¶å†…çš„æ–‡ç« ï¼ˆé»˜è®¤ 48 å°æ—¶ï¼‰

    Returns:
        æ–‡ç« åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
        - title (str): æ–‡ç« æ ‡é¢˜
        - link (str): æ–‡ç«  URL
        - published (str): å‘å¸ƒæ—¶é—´ï¼ˆISO 8601 æ ¼å¼, UTCï¼‰
        - summary (str): çº¯æ–‡æœ¬æ‘˜è¦ï¼ˆæˆªæ–­è‡³ 200 å­—ç¬¦ï¼‰
    """
    logger.info(f"æ­£åœ¨æ‹‰å– .NET åšå®¢ RSS: {DOTNET_FEED_URL}")

    try:
        feed = feedparser.parse(DOTNET_FEED_URL)
    except Exception as e:
        logger.error(f"RSS æ‹‰å–å¼‚å¸¸: {e}")
        return []

    if feed.bozo:
        # bozo=True ä»£è¡¨ RSS æ ¼å¼å­˜åœ¨é—®é¢˜ï¼Œä½†ä¸ä¸€å®šå¯¼è‡´æ•°æ®ä¸¢å¤±ï¼Œä»…è®°å½•è­¦å‘Š
        logger.warning(f"RSS æ ¼å¼è­¦å‘Šï¼ˆbozoï¼‰: {feed.bozo_exception}")

    # è®¡ç®—æˆªæ­¢æ—¶é—´ï¼ˆå½“å‰ UTC æ—¶é—´å¾€å‰æ¨ hours å°æ—¶ï¼‰
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
    articles: list[dict] = []

    for entry in feed.entries:
        # è§£æå‘å¸ƒæ—¶é—´
        pub_dt: datetime | None = None
        if hasattr(entry, "published"):
            try:
                pub_dt = parsedate_to_datetime(entry.published)
                # ç¡®ä¿æœ‰æ—¶åŒºä¿¡æ¯ï¼ˆfeedparser å¤§å¤šæ•°æƒ…å†µä¸‹ä¼šè¿”å›å¸¦æ—¶åŒºçš„æ—¶é—´ï¼‰
                if pub_dt.tzinfo is None:
                    pub_dt = pub_dt.replace(tzinfo=timezone.utc)
            except Exception:
                pass

        # è‹¥æ— æ³•è§£ææ—¶é—´ï¼Œè·³è¿‡è¯¥æ¡ç›®
        if pub_dt is None:
            logger.debug(f"è·³è¿‡æ— æ³•è§£æå‘å¸ƒæ—¶é—´çš„æ¡ç›®: {entry.get('title', 'UNKNOWN')}")
            continue

        # ä»…ä¿ç•™æ—¶é—´çª—å£å†…çš„æ–‡ç« 
        if pub_dt < cutoff:
            continue

        # æå–æ‘˜è¦ï¼ˆsummary å­—æ®µå¯èƒ½å«æœ‰ HTMLï¼Œéœ€å‰¥ç¦»ï¼‰
        raw_summary = entry.get("summary", "") or ""
        clean_summary = _truncate(_strip_html(raw_summary))

        articles.append(
            {
                "title": entry.get("title", "æ— æ ‡é¢˜").strip(),
                "link": entry.get("link", ""),
                "published": pub_dt.strftime("%Y-%m-%d %H:%M UTC"),
                "summary": clean_summary,
            }
        )

    logger.info(f"å…±æ‰¾åˆ° {len(articles)} ç¯‡æœ€è¿‘ {hours} å°æ—¶å†…çš„ .NET åšå®¢æ–‡ç« ")
    return articles


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  æœ¬åœ°è°ƒè¯•å…¥å£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")
    results = fetch_dotnet_articles()
    if results:
        for art in results:
            print(f"\nğŸ“„ {art['title']}")
            print(f"   ğŸ”— {art['link']}")
            print(f"   ğŸ• {art['published']}")
            print(f"   ğŸ“ {art['summary']}")
    else:
        print("å½“å‰æ—¶é—´çª—å£å†…æ²¡æœ‰æ–°æ–‡ç« ï¼ˆå¯å°è¯•å¢å¤§ hours å‚æ•°ï¼‰")
